{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e277e590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting pytorch-crf\n",
      "  Downloading pytorch_crf-0.7.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
      "Installing collected packages: pytorch-crf\n",
      "Successfully installed pytorch-crf-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861630a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "class MaskedAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, lstm_out, mask):\n",
    "        # lstm_out: (batch, seq_len, hidden_dim)\n",
    "        scores = self.attn(lstm_out).squeeze(-1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_weights = torch.softmax(scores, dim=1).unsqueeze(-1)\n",
    "        context = torch.sum(lstm_out * attn_weights, dim=1, keepdim=True)\n",
    "        return lstm_out + context.expand_as(lstm_out)\n",
    "\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim=128, hidden_dim=256, num_layers=2, dropout=0.25, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.bilstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim // 2,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.attention = MaskedAttention(hidden_dim)\n",
    "\n",
    "        # optional hidden projection before CRF\n",
    "        self.hidden_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.crf = CRF(tagset_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x, tags=None, mask=None):\n",
    "        if mask is None:\n",
    "            mask = (x != self.embedding.padding_idx).type(torch.bool)\n",
    "        mask[:, 0] = 1\n",
    "\n",
    "        embeddings = self.embedding(x)\n",
    "        embeddings = self.embedding_dropout(embeddings)\n",
    "\n",
    "        lstm_out, _ = self.bilstm(embeddings)\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        lstm_out = self.attention(lstm_out, mask)\n",
    "        lstm_out = self.hidden_fc(lstm_out)\n",
    "\n",
    "        emissions = self.fc(lstm_out)\n",
    "\n",
    "        if tags is not None:\n",
    "            log_likelihood = self.crf(emissions, tags, mask=mask)\n",
    "            return -log_likelihood.mean()\n",
    "        else:\n",
    "            return self.crf.decode(emissions, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e46d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and dictionaries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "## Test NER model\n",
    "import torch\n",
    "import json\n",
    "from nltk import sent_tokenize\n",
    "import unicodedata\n",
    "\n",
    "with open(\"D:/Study/Education/Projects/Group_Project/source/data/ner/model/token2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    token2idx = json.load(f)\n",
    "\n",
    "with open(\"D:/Study/Education/Projects/Group_Project/source/data/ner/model/label2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    label2idx = json.load(f)\n",
    "\n",
    "#Reverse lookup dict\n",
    "idx2label = {v: k for k, v in label2idx.items()}\n",
    "\n",
    "#Recreate model architecture\n",
    "model = BiLSTM_CRF(\n",
    "    vocab_size=len(token2idx),\n",
    "    tagset_size=len(label2idx),\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    dropout=0.3,\n",
    "    pad_idx=token2idx[\"<PAD>\"]\n",
    ")\n",
    "\n",
    "# Load trained weights \n",
    "model.load_state_dict(torch.load(\"D:/Study/Education/Projects/Group_Project/source/data/ner/model/model_bilstm_crf.pt\", map_location=torch.device(\"cpu\")))\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Model and dictionaries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "053d7042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76c1efbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('luật', 'B-TIT'),\n",
       " ('thuế', 'I-TIT'),\n",
       " ('giá', 'I-TIT'),\n",
       " ('trị', 'I-TIT'),\n",
       " ('gia', 'I-TIT'),\n",
       " ('tăng', 'I-TIT'),\n",
       " ('số', 'O'),\n",
       " ('12', 'B-DOCID'),\n",
       " ('/', 'I-DOCID'),\n",
       " ('23', 'I-DOCID'),\n",
       " ('/', 'I-DOCID'),\n",
       " ('nđ', 'I-DOCID'),\n",
       " ('-', 'O'),\n",
       " ('cp', 'O'),\n",
       " ('được', 'O'),\n",
       " ('ban', 'O'),\n",
       " ('hành', 'O'),\n",
       " ('bởi', 'O'),\n",
       " ('chính', 'B-DEP'),\n",
       " ('phủ', 'I-DEP'),\n",
       " ('và', 'O'),\n",
       " ('được', 'O'),\n",
       " ('kí', 'O'),\n",
       " ('kết', 'O'),\n",
       " ('bởi', 'O'),\n",
       " ('chủ', 'O'),\n",
       " ('tịch', 'O'),\n",
       " ('quốc', 'B-LOC'),\n",
       " ('hội', 'I-LOC'),\n",
       " ('trần', 'I-LOC'),\n",
       " ('thanh', 'I-LOC'),\n",
       " ('mẫn', 'I-LOC'),\n",
       " ('hà', 'I-LOC'),\n",
       " ('nội', 'I-LOC'),\n",
       " ('ngày', 'B-DAT'),\n",
       " ('23', 'I-DAT'),\n",
       " ('tháng', 'I-DAT'),\n",
       " ('8', 'I-DAT'),\n",
       " ('năm', 'I-DAT'),\n",
       " ('2021', 'I-DAT')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def prepare_text_for_model(text, token2idx, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Prepare a raw text string for BiLSTM-CRF inference.\n",
    "    Returns: (X_tensor, mask_tensor, tokens)\n",
    "    \"\"\"\n",
    "    # Normalize and tokenize\n",
    "    text = text.lower().strip()\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
    "\n",
    "    # Convert tokens to indices\n",
    "    X = [token2idx.get(tok, token2idx[\"<UNK>\"]) for tok in tokens]\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_tensor = torch.tensor([X], dtype=torch.long).to(device)\n",
    "    mask = (X_tensor != token2idx[\"<PAD>\"]).to(torch.bool)\n",
    "\n",
    "    return X_tensor, mask, tokens\n",
    "\n",
    "# text = sent_tokenize(try_thong_tu)[0]\n",
    "text = \"Luật Thuế Giá trị gia tăng số 12/23/Nđ-CP được ban hành bởi Chính phủ và được kí kết bởi Chủ tịch QUốc hội Trần Thanh Mẫn Hà Nội ngày 23 tháng 8 năm 2021\"\n",
    "\n",
    "X_tensor, mask, tokens = prepare_text_for_model(text, token2idx, device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(X_tensor, mask=mask)\n",
    "\n",
    "pred_labels = [idx2label[i] for i in pred[0]]\n",
    "list(zip(tokens, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e46242db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "\n",
    "ner_annotator = VnCoreNLP(\"D:/Study/Education/Projects/Group_Project/VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg,pos,ner\", max_heap_size='-Xmx2g') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "193fe733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentences': [[{'index': 1,\n",
       "    'form': 'CHỦ_TỊCH',\n",
       "    'posTag': 'Ny',\n",
       "    'nerLabel': 'O',\n",
       "    'head': -1},\n",
       "   {'index': 2,\n",
       "    'form': 'QUỐC_HỘI',\n",
       "    'posTag': 'Ny',\n",
       "    'nerLabel': 'O',\n",
       "    'head': -1},\n",
       "   {'index': 3,\n",
       "    'form': 'Trần_Thanh_Mẫn',\n",
       "    'posTag': 'Np',\n",
       "    'nerLabel': 'B-PER',\n",
       "    'head': -1}]]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_annotator.annotate(sent_tokenize(try_text)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3264ba",
   "metadata": {},
   "source": [
    "#### Try Data Extraction framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1969e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "#for module import\n",
    "import sys, os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from shared_functions.global_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "326eecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_like_conll(text):\n",
    "    \"\"\"\n",
    "    Split tokens like Label Studio:\n",
    "    - Separate punctuation (/,:; etc.)\n",
    "    - Split numbers, letters, symbols\n",
    "    \"\"\"\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
    "\n",
    "def run_ner_on_sentence(sentence, model, token2idx, idx2label, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Tokenize a sentence, convert to indices (alreay had indices dictionary), run NER model, and return (tokens, labels).\n",
    "    \"\"\"\n",
    "    tokens = tokenize_like_conll(sentence.lower())\n",
    "\n",
    "    # Convert tokens to indices\n",
    "    X = [token2idx.get(tok, token2idx[\"<UNK>\"]) for tok in tokens]\n",
    "    X_tensor = torch.tensor([X], dtype=torch.long).to(device)\n",
    "\n",
    "    # Create attention mask\n",
    "    mask = (X_tensor != token2idx[\"<PAD>\"]).to(torch.bool)\n",
    "\n",
    "    # Predictx label from model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_tensor, mask=mask)[0]  # assume model returns label IDs\n",
    "        preds = preds[:len(tokens)]\n",
    "\n",
    "    labels = [idx2label[p] for p in preds]\n",
    "    \n",
    "    return tokens, labels\n",
    "\n",
    "\n",
    "def merge_entities(tokens, labels):\n",
    "    \"\"\"\n",
    "    Merge consecutive tokens with the same entity type (B-I-O format).\n",
    "    Returns list of (text, entity_type)\n",
    "    Example: Hà - B-LOC, Nội - I-LOC  => (\"Hà Nội\", \"LOC\")\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    current_tokens = []\n",
    "    current_type = None\n",
    "\n",
    "    for tok, lbl in zip(tokens, labels):\n",
    "        if lbl.startswith(\"B-\"):\n",
    "            if current_tokens:\n",
    "                entities.append((\" \".join(current_tokens), current_type))\n",
    "            current_tokens = [tok]\n",
    "            current_type = lbl[2:]\n",
    "        elif lbl.startswith(\"I-\") and current_tokens:\n",
    "            current_tokens.append(tok)\n",
    "        else:\n",
    "            if current_tokens:\n",
    "                entities.append((\" \".join(current_tokens), current_type))\n",
    "                current_tokens = []\n",
    "                current_type = None\n",
    "\n",
    "    if current_tokens:\n",
    "        entities.append((\" \".join(current_tokens), current_type))\n",
    "    return entities\n",
    "\n",
    "\n",
    "# def smart_fix_loc(entities):\n",
    "#     \"\"\"\n",
    "#     Merge common location patterns like:\n",
    "#     'Thành phố' + next LOC, or 'Tỉnh' + next LOC.\n",
    "#     \"\"\"\n",
    "#     fixed = []\n",
    "#     skip_next = False\n",
    "#     for i, (text, etype) in enumerate(entities):\n",
    "#         if skip_next:\n",
    "#             skip_next = False\n",
    "#             continue\n",
    "\n",
    "#         if etype == \"LOC\" and i + 1 < len(entities):\n",
    "#             next_text, next_type = entities[i + 1]\n",
    "#             if next_type == \"LOC\" and text.lower() in [\"thành phố\", \"tỉnh\", \"quận\", \"huyện\"]:\n",
    "#                 fixed.append((f\"{text} {next_text}\", \"LOC\"))\n",
    "#                 skip_next = True\n",
    "#                 continue\n",
    "\n",
    "#         fixed.append((text, etype))\n",
    "#     return fixed\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    text = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "    text = text.replace(\"–\", \"-\").replace(\"_\", \" \")\n",
    "    text = re.sub(r\"[-–—,:;.\\s]+$\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def merge_fragmented(text):\n",
    "    \"\"\"\n",
    "    Merge the token starts with a vowel with the token before it\n",
    "    EXCEPT for certain Vietnamese words\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    vowels = \"aeiouyàáảãạăắằẳẵặâấầẩẫậèéẻẽẹêếềểễệìíỉĩịòóỏõọôốồổỗộơớờởỡợùúủũụưứừửữựýỳỷỹỵ\"\n",
    "    vowel_start = re.compile(rf\"^[{vowels}]\", re.IGNORECASE)\n",
    "\n",
    "    # Words that should *not* be merged (valid Vietnamese words that varies based on use cases)\n",
    "    exclude_words = {\"án\", 'anh'}\n",
    "\n",
    "    parts = text.split()\n",
    "    if len(parts) < 2:\n",
    "        return text\n",
    "\n",
    "    merged = [parts[0]]\n",
    "    for token in parts[1:]:\n",
    "        token_lower = token.lower()\n",
    "        if vowel_start.match(token_lower) and token_lower not in exclude_words:\n",
    "            merged[-1] += token\n",
    "        else:\n",
    "            merged.append(token)\n",
    "\n",
    "    return \" \".join(merged)\n",
    "\n",
    "def normalize_date(text):\n",
    "    \"\"\"\n",
    "    Convert various Vietnamese date formats to 'dd/mm/yyyy'.\n",
    "    Examples:\n",
    "        'ngày 12 tháng 6 năm 2021'  -> '12/06/2021'\n",
    "        '17/6/2025'                 -> '17/06/2025'\n",
    "        '17/6'                      -> '17/06/'\n",
    "        'ngày 5 tháng 12 năm 24'    -> '05/12/2024'\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    text = text.strip().lower()\n",
    "    text = text.replace(\"–\", \"-\").replace(\"_\", \" \")\n",
    "\n",
    "    # Try to match \"ngày 12 tháng 6 năm 2021\"\n",
    "    match_vn = re.search(\n",
    "        r\"ngày\\s*(\\d{1,2})\\D+tháng\\s*(\\d{1,2})\\D+năm\\s*(\\d{2,4})\",\n",
    "        text,\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    if match_vn:\n",
    "        d, m, y = match_vn.groups()\n",
    "        if len(y) == 2:\n",
    "            y = \"20\" + y\n",
    "        return f\"{d.zfill(2)}/{m.zfill(2)}/{y}\"\n",
    "    \n",
    "    # Try to match numeric formats like \"17/6/2025\" or \"17-6-25\" or \"17.6.2025\"\n",
    "    match_num = re.search(r\"(\\d{1,2})[^\\d]+(\\d{1,2})(?:[^\\d]+(\\d{2,4}))?\", text)\n",
    "    if match_num:\n",
    "        d, m, y = match_num.groups()\n",
    "        if y:\n",
    "            if len(y) == 2:\n",
    "                y = \"20\" + y\n",
    "            return f\"{d.zfill(2)}/{m.zfill(2)}/{y}\"\n",
    "        else:\n",
    "            return f\"{d.zfill(2)}/{m.zfill(2)}\"\n",
    "        \n",
    "\n",
    "    # If no recognizable format found, return cleaned text\n",
    "    return re.sub(r\"\\s+\", \" \", text.strip())\n",
    "\n",
    "def extract_abbreviation(text):\n",
    "    def remove_accents(s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "            )\n",
    "\n",
    "    # Normalize and split\n",
    "    parts = remove_accents(text).strip().split()\n",
    "    abbreviation = ''.join(p[0].upper() for p in parts if p)\n",
    "    \n",
    "    return abbreviation\n",
    "\n",
    "def clean_document_id(id_str):\n",
    "    if not isinstance(id_str, str):\n",
    "        return id_str\n",
    "    id_str = id_str.lower().strip()\n",
    "    # Common noise patterns\n",
    "    replacements = {\n",
    "        r'cpu\\b': 'cp',\n",
    "        r'qhu\\b': 'qh',\n",
    "        r'bnvu\\b': 'bnv',\n",
    "        r'bkhu\\b': 'bkh',\n",
    "        r'ttu\\b': 'tt',\n",
    "        r'nhnnvn':'nhnn',\n",
    "        r'bkhvcn':'bkhcn'\n",
    "    }\n",
    "    for pattern, repl in replacements.items():\n",
    "        id_str = re.sub(pattern, repl, id_str)\n",
    "    return id_str\n",
    "\n",
    "def to_upper_alnum(text):\n",
    "    '''\n",
    "    Uppercase all words in a string\n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    return re.sub(r'[a-zA-ZÀ-ỹ]', lambda m: m.group(0).upper(), text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd431770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_document_metadata(text, model, token2idx, idx2label, device=\"cpu\"):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return pd.DataFrame([{}])\n",
    "\n",
    "    first_sentence = sentences[0]\n",
    "    last_sentence = sentences[-1]\n",
    "\n",
    "    tokens, labels = run_ner_on_sentence(first_sentence, model, token2idx, idx2label, device)\n",
    "    entities = merge_entities(tokens, labels)\n",
    "    # entities = smart_fix_loc(entities)\n",
    "\n",
    "    last_tokens, last_labels = run_ner_on_sentence(last_sentence, model, token2idx, idx2label, device)\n",
    "    last_entities = merge_entities(last_tokens, last_labels)\n",
    "\n",
    "    metadata = {\n",
    "        \"issuer_department\": None,\n",
    "        \"issue_date\": None,\n",
    "        \"title\": None,\n",
    "        \"location\": None,\n",
    "        \"document_id\": None,\n",
    "        \"issuer\": None,\n",
    "        \"document_type\": None\n",
    "    }\n",
    "\n",
    "    #Extract entities from first sentence\n",
    "    seen_loc = 0\n",
    "    for text_, etype in entities:\n",
    "        if etype == \"DEP\" and metadata[\"issuer_department\"] is None:\n",
    "            metadata[\"issuer_department\"] = normalize_text(text_)\n",
    "            \n",
    "        elif etype == \"DAT\" and metadata[\"issue_date\"] is None:\n",
    "            metadata[\"issue_date\"] = normalize_date(text_)\n",
    "            \n",
    "        elif etype == \"LOC\":\n",
    "            seen_loc += 1\n",
    "            if seen_loc == 2 and metadata[\"location\"] is None:\n",
    "                metadata[\"location\"] = normalize_text(text_)\n",
    "                \n",
    "        elif etype == \"DOCID\" and metadata[\"document_id\"] is None:\n",
    "            if text_.endswith(\"-\"):\n",
    "                text_ += \"cp\"\n",
    "            elif len(text_.split('/')[-1]) <= 2:\n",
    "                text_ += f'-{extract_abbreviation(metadata[\"issuer_department\"])}'\n",
    "                \n",
    "            #Normalize then remove all spaces between characters\n",
    "            metadata[\"document_id\"] = re.sub(r\"\\s+\", \"\", normalize_text(text_))\n",
    "\n",
    "    #Auto-fill missing year in issue_date\n",
    "    if metadata.get(\"issue_date\"):\n",
    "        date_str = metadata[\"issue_date\"]\n",
    "        if date_str.count('/') == 1:\n",
    "            date_str = f'{date_str}/{metadata[\"document_id\"].split(\"/\")[1]}'\n",
    "            metadata[\"issue_date\"] = date_str\n",
    "\n",
    "    #Default location to Hanoi\n",
    "    if not metadata[\"location\"]:\n",
    "        metadata[\"location\"] = \"Hà Nội\"\n",
    "\n",
    "    #Title & Document Type Extraction \n",
    "    title_keywords = [\"luật\", \"nghị quyết\", \"nghị định\", \"thông tư\", \"quyết định\", \"chỉ thị\", \"hướng dẫn\"]\n",
    "    title_finding =['luật', 'nghị', 'thông', 'quyết', 'chỉ', 'hướng']\n",
    "\n",
    "    # Reassign labels for title-start tokens\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if tok.lower() in title_finding:\n",
    "            labels[i] = \"B-TIT\"\n",
    "\n",
    "    # Find first B-TIT\n",
    "    first_b_tit_idx = next((i for i, lbl in enumerate(labels) if lbl == \"B-TIT\"), None)\n",
    "\n",
    "    if first_b_tit_idx is not None:\n",
    "        start_idx = first_b_tit_idx\n",
    "        end_idx = start_idx\n",
    "\n",
    "        for k in range(start_idx + 1, len(tokens)):\n",
    "            if labels[k].endswith(\"TIT\"):\n",
    "                end_idx = k\n",
    "            else:\n",
    "                if (end_idx - start_idx + 1) < 5:\n",
    "                    end_idx = min(start_idx + 4, len(tokens) - 1)\n",
    "                break\n",
    "\n",
    "        candidate_title = \" \".join(tokens[start_idx:end_idx + 1])\n",
    "        candidate_title = normalize_text(candidate_title)\n",
    "        metadata[\"title\"] = candidate_title\n",
    "\n",
    "        # Extract document type from title keyword\n",
    "        for kw in title_keywords:\n",
    "            if candidate_title.lower().startswith(kw):\n",
    "                metadata[\"document_type\"] = kw.capitalize()\n",
    "                break\n",
    "    else:\n",
    "        metadata[\"title\"] = \"UNKNOWN\"\n",
    "        metadata[\"document_type\"] = \"UNKNOWN\"\n",
    "\n",
    "    # Second “Luật” occurrence logic\n",
    "    if metadata[\"document_type\"] == \"Luật\":\n",
    "        luat_indices = [i for i, t in enumerate(tokens) if t.lower() == \"luật\"]\n",
    "        if len(luat_indices) >= 2:\n",
    "            start_idx = luat_indices[1]\n",
    "            end_idx = start_idx\n",
    "            for k in range(start_idx + 1, len(tokens)):\n",
    "                if labels[k].endswith(\"TIT\"):\n",
    "                    end_idx = k\n",
    "                else:\n",
    "                    if (end_idx - start_idx + 1) < 5:\n",
    "                        end_idx = min(start_idx + 4, len(tokens) - 1)\n",
    "                    break\n",
    "            metadata[\"title\"] = normalize_text(\" \".join(tokens[start_idx:end_idx + 1]))\n",
    "\n",
    "    #Ensure consistency between title and document_type \n",
    "    if metadata[\"title\"] and metadata[\"document_type\"]:\n",
    "        title_lower = metadata[\"title\"].lower()\n",
    "        doc_type_lower = metadata[\"document_type\"].lower()\n",
    "        if not title_lower.startswith(doc_type_lower):\n",
    "            metadata[\"title\"] = f\"{metadata['document_type']} {metadata['title']}\"\n",
    "    elif metadata[\"title\"]:\n",
    "        for kw in title_keywords:\n",
    "            if metadata[\"title\"].lower().startswith(kw):\n",
    "                metadata[\"document_type\"] = kw.capitalize()\n",
    "                break\n",
    "\n",
    "    ## Extract Issuer\n",
    "    try:\n",
    "        annotated = ner_annotator.annotate(last_sentence)\n",
    "        persons = []\n",
    "        current_name = []\n",
    "\n",
    "        for sent in annotated[\"sentences\"]:\n",
    "            for word_info in sent:\n",
    "                label = word_info.get(\"nerLabel\")\n",
    "                token = word_info.get(\"form\", \"\").replace(\"_\", \" \")\n",
    "                if label == \"B-PER\":\n",
    "                    if current_name:\n",
    "                        persons.append(\" \".join(current_name))\n",
    "                    current_name = [token]\n",
    "                elif label == \"I-PER\" and current_name:\n",
    "                    current_name.append(token)\n",
    "                else:\n",
    "                    if current_name:\n",
    "                        persons.append(\" \".join(current_name))\n",
    "                        current_name = []\n",
    "        if current_name:\n",
    "            persons.append(\" \".join(current_name))\n",
    "        \n",
    "        # Use the last complete PER entity\n",
    "        if persons:\n",
    "            metadata[\"issuer\"] = normalize_text(persons[-1])\n",
    "        else:\n",
    "            metadata[\"issuer\"] = \"UNKNOWN\"\n",
    "\n",
    "        if len(metadata[\"issuer\"].split(' ')) == 1:\n",
    "            metadata[\"issuer\"] = \"UNKNOWN\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] VnCoreNLP extraction failed: {e}\")\n",
    "        metadata[\"issuer\"] = \"UNKNOWN\"\n",
    "\n",
    "    # Capitalization Fix and final check\n",
    "    df = pd.DataFrame([metadata])\n",
    "    \n",
    "    #Fix missing document id\n",
    "    mask = df['document_id'].str.split('/').str[-1].apply(lambda x: len(str(x).strip()) <= 2)\n",
    "\n",
    "    df.loc[mask, 'document_id'] = (\n",
    "        df.loc[mask, 'document_id'] + '-' +\n",
    "        df.loc[mask, 'issuer_department'].map(str.strip).apply(extract_abbreviation).str.lower()\n",
    "    )\n",
    "    \n",
    "    df['document_id'] = df['document_id'].apply(clean_document_id)\n",
    "    \n",
    "    #Fix some fragmented features\n",
    "    df = df.applymap(merge_fragmented)\n",
    "    \n",
    "    cols_to_capitalize = ['issuer_department', 'title', 'location', 'issuer', 'document_type']\n",
    "    df[cols_to_capitalize] = df[cols_to_capitalize].apply(\n",
    "        lambda col: col.apply(lambda x: x.title() if isinstance(x, str) else x)\n",
    "    )\n",
    "    \n",
    "    #Capitalize all letters in document_id\n",
    "    df['document_id'] = df['document_id'].apply(to_upper_alnum)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b87f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'D:/Study/Education/Projects/Group_Project/source/document'\n",
    "files = os.listdir(file_path)\n",
    "\n",
    "list_doc_content = []\n",
    "for file in files:\n",
    "    if not file.lower().endswith(('.pdf', '.docx', '.doc')):\n",
    "        continue\n",
    "    list_doc_content.append(get_text_from_s3(f'legaldocstorage/{file}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da874729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(merge_fragmented)\n"
     ]
    }
   ],
   "source": [
    "# import botocore\n",
    "\n",
    "# for file in files:\n",
    "#     try:\n",
    "#         try_text = get_text_from_s3(f'legaldocstorage/{file}')\n",
    "#         df_meta = extract_document_metadata(try_text, model, token2idx, idx2label, device)\n",
    "#         df = pd.concat([df, df_meta], axis=0)\n",
    "#     except botocore.exceptions.ClientError as e:\n",
    "#         if e.response['Error']['Code'] == 'NoSuchKey':\n",
    "#             print(f\"❌ NoSuchKey error: File not found in S3 → {file}\")\n",
    "#         else:\n",
    "#             print(f\"⚠️ Other S3 error for file {file}: {e}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"⚠️ Unexpected error with file {file}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(columns = ['issuer_department', 'issue_date', 'title', 'location', 'document_id', 'issuer', 'document_type'])\n",
    "\n",
    "for doc_content in list_doc_content:\n",
    "    df_meta = extract_document_metadata(doc_content, model, token2idx, idx2label, device)\n",
    "    df = pd.concat([df, df_meta], axis=0)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "30c6ac10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issuer_department</th>\n",
       "      <th>issue_date</th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>document_id</th>\n",
       "      <th>issuer</th>\n",
       "      <th>document_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quốc Hội</td>\n",
       "      <td>17/06/2020</td>\n",
       "      <td>Luật Doanh Nghiệp Căn Cứ</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>59/2020/QH14</td>\n",
       "      <td>Nguyễn Thị Kim Ngân</td>\n",
       "      <td>Luật</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quốc Hội</td>\n",
       "      <td>17/06/2025</td>\n",
       "      <td>Luật Sửa Đổi , Bổ</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>76/2025/QH15</td>\n",
       "      <td>Trần Thanh Mẫn</td>\n",
       "      <td>Luật</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quốc Hội</td>\n",
       "      <td>26/11/2024</td>\n",
       "      <td>Luật Thuế Giá Trị Gia Tăng</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>48/2024/QH15</td>\n",
       "      <td>Trần Thanh Mẫn</td>\n",
       "      <td>Luật</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quốc Hội</td>\n",
       "      <td>06/04/2016</td>\n",
       "      <td>Luật Sửa Đổi , Bổ Sung Một Số Điều</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>106/2016/QH13</td>\n",
       "      <td>Nguyễn Thị Kim Ngân</td>\n",
       "      <td>Luật</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quốc Hội</td>\n",
       "      <td>14/06/2025</td>\n",
       "      <td>Luật Thuế Thu Nhập Doanh</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>67/2025/QH15</td>\n",
       "      <td>Trần Thanh Mẫn</td>\n",
       "      <td>Luật</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Quốc Hội</td>\n",
       "      <td>14/06/2025</td>\n",
       "      <td>Luật Thuế Tiêu Thụ Đặc Biệt</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>66/2025/QH15</td>\n",
       "      <td>Trần Thanh Mẫn</td>\n",
       "      <td>Luật</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Quốc Hội</td>\n",
       "      <td>06/04/2016</td>\n",
       "      <td>Luật Thuế Xuất Khẩu , Thuế Nhập Khẩu</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>107/2016/QH13</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Luật</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chính Phủ</td>\n",
       "      <td>03/10/2025</td>\n",
       "      <td>Nghị Định Quy Định Chi Tiết Về Việc Thực Hiện ...</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>256/2025/NĐ-CP</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Nghị Định</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chính Phủ</td>\n",
       "      <td>02/04/2025</td>\n",
       "      <td>Nghị Định Gia Hạn Thời Hạn Nộp Thuế Giá Trị Gi...</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>82/2025/NĐ-CP</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Nghị Định</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chính Phủ</td>\n",
       "      <td>01/07/2025</td>\n",
       "      <td>Nghị Định Quy Định Chi Tiết Thi Hành Một Số Điều</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>181/2025/NĐ-CP</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Nghị Định</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chính Phủ</td>\n",
       "      <td>26/09/2025</td>\n",
       "      <td>Nghị Định Quy Định Về</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>254/2025/NĐ-CP</td>\n",
       "      <td>Văn Bản</td>\n",
       "      <td>Nghị Định</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chính Phủ</td>\n",
       "      <td>08/07/2025</td>\n",
       "      <td>Nghị Định Sửa Đổi , Bổ Sung Nghị Định</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>199/2025/NĐ-CP</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Nghị Định</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chính Phủ</td>\n",
       "      <td>08/06/2024</td>\n",
       "      <td>Nghị Quyết Về Dự Án Nghị Quyết Của Quốc Hội Về...</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>83/NQ-CP</td>\n",
       "      <td>Lê Minh Khái</td>\n",
       "      <td>Nghị Quyết</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ủy Ban Nhân Dân</td>\n",
       "      <td>01/10/2025</td>\n",
       "      <td>Quyết Định Ban Hành Quy</td>\n",
       "      <td>Cà Mau</td>\n",
       "      <td>034/2025/QĐ-UBND</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Quyết Định</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bộ Khoa Học Và Công Nghệ</td>\n",
       "      <td>02/10/2025</td>\n",
       "      <td>Quyết Định Ban Hành Kế</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>3004/QĐ-BKHCN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Quyết Định</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bộ Nội Vụ</td>\n",
       "      <td>03/10/2025</td>\n",
       "      <td>Quyết Định Về Việc Công</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>1136/QĐ-BNV</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Quyết Định</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bộ Khoa Học Và Công Nghệ</td>\n",
       "      <td>01/10/2025</td>\n",
       "      <td>Quyết Định Ban Hành Kế</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>2985/QĐ-BKHCN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Quyết Định</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bộ Tài Chính</td>\n",
       "      <td>03/10/2025</td>\n",
       "      <td>Thông Tư Bãi Bỏ Thông</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>92/2025/TT-BTC</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Thông Tư</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Bộ Tài Chính</td>\n",
       "      <td>23/07/2024</td>\n",
       "      <td>Thông Tư Bãi Bỏ Một</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>52/2024/TT-BTC</td>\n",
       "      <td>Cao Anh Tuấn</td>\n",
       "      <td>Thông Tư</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ngân Hàng Nhà Nước Việt Nam</td>\n",
       "      <td>30/09/2025</td>\n",
       "      <td>Thông Tư Quy Định Về</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>31/2025/TT-NHNN</td>\n",
       "      <td>Đoàn Thái Sơn</td>\n",
       "      <td>Thông Tư</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bộ Quốc Phòng</td>\n",
       "      <td>02/10/2025</td>\n",
       "      <td>Thông Tư Quy Định Tặng</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>109/2025/TT-BQP</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Thông Tư</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              issuer_department  issue_date  \\\n",
       "0                      Quốc Hội  17/06/2020   \n",
       "1                      Quốc Hội  17/06/2025   \n",
       "2                      Quốc Hội  26/11/2024   \n",
       "3                      Quốc Hội  06/04/2016   \n",
       "4                      Quốc Hội  14/06/2025   \n",
       "5                      Quốc Hội  14/06/2025   \n",
       "6                      Quốc Hội  06/04/2016   \n",
       "7                     Chính Phủ  03/10/2025   \n",
       "8                     Chính Phủ  02/04/2025   \n",
       "9                     Chính Phủ  01/07/2025   \n",
       "10                    Chính Phủ  26/09/2025   \n",
       "11                    Chính Phủ  08/07/2025   \n",
       "12                    Chính Phủ  08/06/2024   \n",
       "13              Ủy Ban Nhân Dân  01/10/2025   \n",
       "14     Bộ Khoa Học Và Công Nghệ  02/10/2025   \n",
       "15                    Bộ Nội Vụ  03/10/2025   \n",
       "16     Bộ Khoa Học Và Công Nghệ  01/10/2025   \n",
       "17                 Bộ Tài Chính  03/10/2025   \n",
       "18                 Bộ Tài Chính  23/07/2024   \n",
       "19  Ngân Hàng Nhà Nước Việt Nam  30/09/2025   \n",
       "20                Bộ Quốc Phòng  02/10/2025   \n",
       "\n",
       "                                                title location  \\\n",
       "0                            Luật Doanh Nghiệp Căn Cứ   Hà Nội   \n",
       "1                                   Luật Sửa Đổi , Bổ   Hà Nội   \n",
       "2                          Luật Thuế Giá Trị Gia Tăng   Hà Nội   \n",
       "3                  Luật Sửa Đổi , Bổ Sung Một Số Điều   Hà Nội   \n",
       "4                            Luật Thuế Thu Nhập Doanh   Hà Nội   \n",
       "5                         Luật Thuế Tiêu Thụ Đặc Biệt   Hà Nội   \n",
       "6                Luật Thuế Xuất Khẩu , Thuế Nhập Khẩu   Hà Nội   \n",
       "7   Nghị Định Quy Định Chi Tiết Về Việc Thực Hiện ...   Hà Nội   \n",
       "8   Nghị Định Gia Hạn Thời Hạn Nộp Thuế Giá Trị Gi...   Hà Nội   \n",
       "9    Nghị Định Quy Định Chi Tiết Thi Hành Một Số Điều   Hà Nội   \n",
       "10                              Nghị Định Quy Định Về   Hà Nội   \n",
       "11              Nghị Định Sửa Đổi , Bổ Sung Nghị Định   Hà Nội   \n",
       "12  Nghị Quyết Về Dự Án Nghị Quyết Của Quốc Hội Về...   Hà Nội   \n",
       "13                            Quyết Định Ban Hành Quy   Cà Mau   \n",
       "14                             Quyết Định Ban Hành Kế   Hà Nội   \n",
       "15                            Quyết Định Về Việc Công   Hà Nội   \n",
       "16                             Quyết Định Ban Hành Kế   Hà Nội   \n",
       "17                              Thông Tư Bãi Bỏ Thông   Hà Nội   \n",
       "18                                Thông Tư Bãi Bỏ Một   Hà Nội   \n",
       "19                               Thông Tư Quy Định Về   Hà Nội   \n",
       "20                             Thông Tư Quy Định Tặng   Hà Nội   \n",
       "\n",
       "         document_id               issuer document_type  \n",
       "0       59/2020/QH14  Nguyễn Thị Kim Ngân          Luật  \n",
       "1       76/2025/QH15       Trần Thanh Mẫn          Luật  \n",
       "2       48/2024/QH15       Trần Thanh Mẫn          Luật  \n",
       "3      106/2016/QH13  Nguyễn Thị Kim Ngân          Luật  \n",
       "4       67/2025/QH15       Trần Thanh Mẫn          Luật  \n",
       "5       66/2025/QH15       Trần Thanh Mẫn          Luật  \n",
       "6      107/2016/QH13              Unknown          Luật  \n",
       "7     256/2025/NĐ-CP              Unknown     Nghị Định  \n",
       "8      82/2025/NĐ-CP              Unknown     Nghị Định  \n",
       "9     181/2025/NĐ-CP              Unknown     Nghị Định  \n",
       "10    254/2025/NĐ-CP              Văn Bản     Nghị Định  \n",
       "11    199/2025/NĐ-CP              Unknown     Nghị Định  \n",
       "12          83/NQ-CP         Lê Minh Khái    Nghị Quyết  \n",
       "13  034/2025/QĐ-UBND              Unknown    Quyết Định  \n",
       "14     3004/QĐ-BKHCN              Unknown    Quyết Định  \n",
       "15       1136/QĐ-BNV              Unknown    Quyết Định  \n",
       "16     2985/QĐ-BKHCN              Unknown    Quyết Định  \n",
       "17    92/2025/TT-BTC              Unknown      Thông Tư  \n",
       "18    52/2024/TT-BTC         Cao Anh Tuấn      Thông Tư  \n",
       "19   31/2025/TT-NHNN        Đoàn Thái Sơn      Thông Tư  \n",
       "20   109/2025/TT-BQP              Unknown      Thông Tư  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "48cef937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_text = get_text_from_s3('legaldocstorage/luat_thue_tndn_2025.pdf')\n",
    "\n",
    "df_meta = extract_document_metadata(try_text, model, token2idx, idx2label, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9ca9840e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issuer_department</th>\n",
       "      <th>issue_date</th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>document_id</th>\n",
       "      <th>issuer</th>\n",
       "      <th>document_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quốc Hội</td>\n",
       "      <td>14/06/2025</td>\n",
       "      <td>Luật Thuế Thu Nhập Doanh</td>\n",
       "      <td>Hà Nội</td>\n",
       "      <td>67/2025/qh15</td>\n",
       "      <td>Trần Thanh Mẫn</td>\n",
       "      <td>Luật</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  issuer_department  issue_date                     title location  \\\n",
       "0          Quốc Hội  14/06/2025  Luật Thuế Thu Nhập Doanh   Hà Nội   \n",
       "\n",
       "    document_id          issuer document_type  \n",
       "0  67/2025/qh15  Trần Thanh Mẫn          Luật  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0298d1a",
   "metadata": {},
   "source": [
    "#### Hierarchical Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "331ce4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def parse_legal_text(text: str):\n",
    "    # Regex patterns\n",
    "    chapter_pattern = r\"^\\s*Chương\\s+([IVXLCDM\\d]+)\\.?\"\n",
    "    clause_pattern = r\"^\\s*Điều\\s+(\\d+)\\.?\"\n",
    "    point_pattern = r\"^\\s*\\+?\\s*(\\d+)\\.\"\n",
    "    subpoint_pattern = r\"^\\s*(?:\\*?\\s*)?([a-z])\\)\"\n",
    "\n",
    "    # Storage\n",
    "    structure = defaultdict(list)\n",
    "    current_chapter = None\n",
    "    current_clause = None\n",
    "    current_point = None\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Match Chapter\n",
    "        match_chapter = re.match(chapter_pattern, line, flags=re.IGNORECASE)\n",
    "        if match_chapter:\n",
    "            current_chapter = f\"chapter {match_chapter.group(1)}\"\n",
    "            structure[current_chapter] = []\n",
    "            current_clause = None\n",
    "            current_point = None\n",
    "            continue\n",
    "\n",
    "        # Match Clause\n",
    "        match_clause = re.match(clause_pattern, line)\n",
    "        if match_clause:\n",
    "            if not current_chapter:\n",
    "                # If no chapter seen yet → use default top level\n",
    "                current_chapter = \"no_chapter\"\n",
    "                structure[current_chapter] = []\n",
    "            current_clause = {\n",
    "                \"clause\": match_clause.group(1),\n",
    "                \"text\": line,\n",
    "                \"points\": []\n",
    "            }\n",
    "            structure[current_chapter].append(current_clause)\n",
    "            current_point = None\n",
    "            continue\n",
    "\n",
    "        # Match Point\n",
    "        match_point = re.match(point_pattern, line)\n",
    "        if match_point and current_clause:\n",
    "            current_point = {\n",
    "                \"point\": match_point.group(1),\n",
    "                \"text\": line,\n",
    "                \"subpoints\": []\n",
    "            }\n",
    "            current_clause[\"points\"].append(current_point)\n",
    "            continue\n",
    "\n",
    "        # Match Subpoint\n",
    "        match_subpoint = re.match(subpoint_pattern, line, flags=re.IGNORECASE)\n",
    "        if match_subpoint and current_point:\n",
    "            sub_val = match_subpoint.group(1)\n",
    "            current_point[\"subpoints\"].append({\n",
    "                \"subpoint\": sub_val,\n",
    "                \"text\": line\n",
    "            })\n",
    "            continue\n",
    "\n",
    "    return dict(structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3bc2fbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no_chapter': [{'clause': '1', 'text': 'Điều 1. Phạm vi điều chỉnh', 'points': []}, {'clause': '2', 'text': 'Điều 2. Đối tượng áp dụng', 'points': []}, {'clause': '3', 'text': 'Điều 3. Đối tượng được gia hạn', 'points': [{'point': '1', 'text': '1. Doanh nghiệp, tổ chức, hộ gia đình, hộ kinh doanh, cá nhân hoạt động sản xuất trong các ngành kinh', 'subpoints': [{'subpoint': 'a', 'text': 'a) Nông nghiệp, lâm nghiệp và thủy sản;'}, {'subpoint': 'b', 'text': 'b) Sản xuất, chế biến thực phẩm; dệt; sản xuất trang phục; sản xuất da và các sản phẩm có liên quan;'}, {'subpoint': 'c', 'text': 'c) Xây dựng;'}, {'subpoint': 'd', 'text': 'd) Hoạt động xuất bản; hoạt động điện ảnh, sản xuất chương trình truyền hình, ghi âm và xuất bản âm'}, {'subpoint': 'e', 'text': 'e) Sản xuất đồ uống; in, sao chép bản ghi các loại; sản xuất than cốc, sản phẩm dầu mỏ tinh chế; sản'}, {'subpoint': 'g', 'text': 'g) Thoát nước và xử lý nước thải.'}]}, {'point': '2', 'text': '2. Doanh nghiệp, tổ chức, hộ gia đình, hộ kinh doanh, cá nhân hoạt động kinh doanh trong các ngành', 'subpoints': [{'subpoint': 'a', 'text': 'a) Vận tải kho bãi; dịch vụ lưu trú và ăn uống; giáo dục và đào tạo; y tế và hoạt động trợ giúp xã hội;'}, {'subpoint': 'b', 'text': 'b) Hoạt động dịch vụ lao động và việc làm; hoạt động của các đại lý du lịch, kinh doanh tua du lịch và'}, {'subpoint': 'c', 'text': 'c) Hoạt động sáng tác, nghệ thuật và giải trí; hoạt động của thư viện, lưu trữ, bảo tàng và các hoạt động'}, {'subpoint': 'd', 'text': 'd) Hoạt động phát thanh, truyền hình; lập trình máy vi tính, dịch vụ tư vấn và các hoạt động khác liên'}]}, {'point': '3', 'text': '3. Doanh nghiệp, tổ chức, hộ gia đình, hộ kinh doanh, cá nhân hoạt động sản xuất sản phẩm công', 'subpoints': []}, {'point': '4', 'text': '4. Doanh nghiệp nhỏ và siêu nhỏ được xác định theo quy định của Luật Hỗ trợ doanh nghiệp nhỏ và', 'subpoints': []}]}, {'clause': '4', 'text': 'Điều 4. Gia hạn thời hạn nộp thuế và tiền thuê đất', 'points': [{'point': '1', 'text': '1. Đối với thuế giá trị gia tăng (trừ thuế giá trị gia tăng khâu nhập khẩu)', 'subpoints': [{'subpoint': 'a', 'text': 'a) Gia hạn thời hạn nộp thuế đối với số thuế giá trị gia tăng phát sinh phải nộp (bao gồm cả số thuế'}]}, {'point': '2025', 'text': '2025.', 'subpoints': []}, {'point': '2025', 'text': '2025.', 'subpoints': [{'subpoint': 'b', 'text': 'b) Trường hợp doanh nghiệp, tổ chức nêu tại Điều 3 Nghị định này  có các chi nhánh, đơn vị trực thuộc'}]}, {'point': '2', 'text': '2. Đối với thuế thu nhập doanh nghiệp', 'subpoints': [{'subpoint': 'a', 'text': 'a) Gia hạn thời hạn nộp thuế đối với số thuế thu nhập doanh nghiệp tạm nộp của quý I và quý II kỳ tính'}]}]}, {'clause': '3', 'text': 'Điều 3 Nghị định này . Thời gian gia hạn là 05 tháng, kể từ ngày kết thúc thời hạn nộp thuế thu nhập', 'points': [{'point': '3', 'text': '3. Đối với thuế giá trị gia tăng, thuế thu nhập cá nhân của hộ kinh doanh, cá nhân kinh doanh', 'subpoints': []}, {'point': '4', 'text': '4. Đối với tiền thuê đất', 'subpoints': []}, {'point': '5', 'text': '5. Trường hợp doanh nghiệp, tổ chức, hộ kinh doanh, cá nhân kinh doanh có hoạt động sản xuất, kinh', 'subpoints': []}]}, {'clause': '5', 'text': 'Điều 5. Trình tự, thủ tục gia hạn', 'points': [{'point': '1', 'text': '1. Người nộp thuế trực tiếp kê khai, nộp thuế với cơ quan thuế thuộc đối tượng được gia hạn gửi Giấy', 'subpoints': []}, {'point': '2', 'text': '2. Người nộp thuế tự xác định và chịu trách nhiệm về việc đề nghị gia hạn đảm bảo đúng đối tượng', 'subpoints': []}, {'point': '3', 'text': '3. Cơ quan thuế không phải thông báo cho người nộp thuế về việc chấp nhận gia hạn nộp thuế và tiền', 'subpoints': []}, {'point': '4', 'text': '4. Không tính tiền chậm nộp đối với số tiền thuế, tiền thuê đất được gia hạn trong khoảng thời gian', 'subpoints': []}, {'point': '5', 'text': '5. Chủ đầu tư các công trình, hạng mục công trình xây dựng cơ bản bằng nguồn vốn ngân sách nhà', 'subpoints': []}]}, {'clause': '6', 'text': 'Điều 6. Tổ chức thực hiện và hiệu lực thi hành', 'points': [{'point': '1', 'text': '1. Nghị định này có hiệu lực từ ngày ký ban hành đến hết ngày 31 tháng 12 năm 2025.', 'subpoints': []}, {'point': '2', 'text': '2. Sau thời gian gia hạn theo Nghị định này, thời hạn nộp thuế và tiền thuê đất được thực hiện theo quy', 'subpoints': []}, {'point': '3', 'text': '3. Bộ Tài chính chịu trách nhiệm chỉ đạo, tổ chức triển khai và xử lý vướng mắc phát sinh trong quá', 'subpoints': []}, {'point': '4', 'text': '4. Các Bộ trưởng, Thủ trưởng cơ quan ngang bộ, Thủ trưởng cơ quan thuộc Chính phủ, Chủ tịch Ủy', 'subpoints': []}]}]}\n"
     ]
    }
   ],
   "source": [
    "try_dict = parse_legal_text(try_text)\n",
    "\n",
    "print(try_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ca2c664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['issuer_department', 'issue_date', 'title', 'location', 'document_id',\n",
       "       'issuer', 'document_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689523e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_to_neo4j(text, namespace=\"Test\"):\n",
    "    \"\"\"\n",
    "    Save extracted legal document structure and metadata into Neo4j,\n",
    "    under a given namespace label (e.g., :Test1, :Test2)\n",
    "    \"\"\"\n",
    "    # === Extract document metadata ===\n",
    "    df_meta = extract_document_metadata(text, model, token2idx, idx2label, device)\n",
    "    meta_row = df_meta.iloc[0]\n",
    "\n",
    "    metadata = {\n",
    "        \"law_id\": meta_row[\"document_id\"],\n",
    "        \"title\": meta_row[\"title\"],\n",
    "        \"issuer\": meta_row[\"issuer\"],\n",
    "        \"issue_date\": meta_row[\"issue_date\"],\n",
    "        \"location\": meta_row[\"location\"],\n",
    "        \"issuer_department\": meta_row[\"issuer_department\"],\n",
    "        \"document_type\": meta_row[\"document_type\"],\n",
    "        \"amendment_history\": \"None\",\n",
    "        \"jurisdiction_scope\": \"National\"\n",
    "    }\n",
    "\n",
    "    # === Central Node ===\n",
    "    doc_type_label = metadata[\"document_type\"].replace(\" \", \"_\").capitalize()\n",
    "    ns_label = re.sub(r\"\\W+\", \"_\", namespace)  # sanitize label (only letters/numbers/_)\n",
    "\n",
    "    dml_ddl_neo4j(f\"\"\"\n",
    "        MERGE (l:{doc_type_label}:{ns_label} {{law_id: $law_id}})\n",
    "        SET l += $meta\n",
    "    \"\"\", law_id=metadata[\"law_id\"], meta=metadata)\n",
    "\n",
    "    # === Parse document structure ===\n",
    "    parsed = parse_legal_text(text)\n",
    "\n",
    "    ## If document has no chapter\n",
    "    \n",
    "    if not any(k.lower().startswith(\"chapter\") for k in parsed.keys()):\n",
    "        for clause in parsed.values():\n",
    "            if isinstance(clause, list):\n",
    "                for cl in clause:\n",
    "                    clause_id = f\"{metadata['law_id']}_C{cl['clause']}\"\n",
    "                    dml_ddl_neo4j(f\"\"\"\n",
    "                        MERGE (c:Clause:{ns_label} {{id: $id}})\n",
    "                        SET c.text = $text\n",
    "                        WITH c\n",
    "                        MATCH (l:{ns_label} {{law_id: $law_id}})\n",
    "                        MERGE (l)-[:HAS_CLAUSE]->(c)\n",
    "                    \"\"\", id=clause_id, text=cl[\"text\"], law_id=metadata[\"law_id\"])\n",
    "\n",
    "                    for point in cl[\"points\"]:\n",
    "                        point_id = f\"{clause_id}_P{point['point']}\"\n",
    "                        dml_ddl_neo4j(f\"\"\"\n",
    "                            MERGE (p:Point:{ns_label} {{id: $id}})\n",
    "                            SET p.text = $text\n",
    "                            WITH p\n",
    "                            MATCH (c:Clause:{ns_label} {{id: $clause_id}})\n",
    "                            MERGE (c)-[:HAS_POINT]->(p)\n",
    "                        \"\"\", id=point_id, text=point[\"text\"], clause_id=clause_id)\n",
    "\n",
    "                        for subpoint in point[\"subpoints\"]:\n",
    "                            subpoint_id = f\"{point_id}_SP{subpoint['subpoint']}\"\n",
    "                            dml_ddl_neo4j(f\"\"\"\n",
    "                                MERGE (sp:Subpoint:{ns_label} {{id: $id}})\n",
    "                                SET sp.text = $text\n",
    "                                WITH sp\n",
    "                                MATCH (p:Point:{ns_label} {{id: $point_id}})\n",
    "                                MERGE (p)-[:HAS_SUBPOINT]->(sp)\n",
    "                            \"\"\", id=subpoint_id, text=subpoint[\"text\"], point_id=point_id)\n",
    "\n",
    "    # If document has chapters\n",
    "    else:\n",
    "        for chapter, clauses in parsed.items():\n",
    "            chapter_id = f\"{metadata['law_id']}_{chapter.replace(' ', '_')}\"\n",
    "            dml_ddl_neo4j(f\"\"\"\n",
    "                MERGE (ch:Chapter:{ns_label} {{id: $id}})\n",
    "                SET ch.title = $chapter\n",
    "                WITH ch\n",
    "                MATCH (l:{ns_label} {{law_id: $law_id}})\n",
    "                MERGE (l)-[:HAS_CHAPTER]->(ch)\n",
    "            \"\"\", id=chapter_id, chapter=chapter, law_id=metadata[\"law_id\"])\n",
    "\n",
    "            for clause in clauses:\n",
    "                clause_id = f\"{chapter_id}_C{clause['clause']}\"\n",
    "                dml_ddl_neo4j(f\"\"\"\n",
    "                    MERGE (c:Clause:{ns_label} {{id: $id}})\n",
    "                    SET c.text = $text\n",
    "                    WITH c\n",
    "                    MATCH (ch:Chapter:{ns_label} {{id: $chapter_id}})\n",
    "                    MERGE (ch)-[:HAS_CLAUSE]->(c)\n",
    "                \"\"\", id=clause_id, text=clause[\"text\"], chapter_id=chapter_id)\n",
    "\n",
    "                for point in clause[\"points\"]:\n",
    "                    point_id = f\"{clause_id}_P{point['point']}\"\n",
    "                    dml_ddl_neo4j(f\"\"\"\n",
    "                        MERGE (p:Point:{ns_label} {{id: $id}})\n",
    "                        SET p.text = $text\n",
    "                        WITH p\n",
    "                        MATCH (c:Clause:{ns_label} {{id: $clause_id}})\n",
    "                        MERGE (c)-[:HAS_POINT]->(p)\n",
    "                    \"\"\", id=point_id, text=point[\"text\"], clause_id=clause_id)\n",
    "\n",
    "                    for subpoint in point[\"subpoints\"]:\n",
    "                        subpoint_id = f\"{point_id}_SP{subpoint['subpoint']}\"\n",
    "                        dml_ddl_neo4j(f\"\"\"\n",
    "                            MERGE (sp:Subpoint:{ns_label} {{id: $id}})\n",
    "                            SET sp.text = $text\n",
    "                            WITH sp\n",
    "                            MATCH (p:Point:{ns_label} {{id: $point_id}})\n",
    "                            MERGE (p)-[:HAS_SUBPOINT]->(sp)\n",
    "                        \"\"\", id=subpoint_id, text=subpoint[\"text\"], point_id=point_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "00c65340",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_doc = list_doc_content[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "77691629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(truncated_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f668350",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in truncated_doc:\n",
    "    saving_to_neo4j(doc, namespace = \"Test1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
