{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8458d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "d_model = 512\n",
    "\n",
    "W_Q = nn.Linear(d_model, d_model)\n",
    "W_K = nn.Linear(d_model, d_model)\n",
    "W_V = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da92e27",
   "metadata": {},
   "source": [
    "Linear is a linear combination of the input tensor with size (512, 512) that takes input X (batch_size, seq_len, hidden_dim) and conduct a transformation $$y = x.W^T + b$$\n",
    "\n",
    "where W, b are the trainable Weight/ bias matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8495891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:  batch of 2 sentences, each 5 tokens, embedding size 512\n",
    "batch_size = 10\n",
    "seq_len = 5\n",
    "d_model = 512\n",
    "\n",
    "X = torch.randn(batch_size, seq_len, d_model)  # (10, 5, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2094916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = W_Q(X)\n",
    "K = W_K(X)\n",
    "V = W_V(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71543a1c",
   "metadata": {},
   "source": [
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}V)$$\n",
    "\n",
    "Q: Query matrix\n",
    "\n",
    "K: Key matrix\n",
    "\n",
    "V: value matrix\n",
    "\n",
    "$d_k$: dimensions of K\n",
    "\n",
    "Attention also contains 2 types:\n",
    "\n",
    "- Dot-product attention: Simply conduct dot product betweem Query and Key to evaluate the similarity, this is considered faster and lighter to process\n",
    "\n",
    "- Additive attention: Computes the similarity by a feed-forward network with a single hidden layer, which is slower and more costly in computation\n",
    "\n",
    "In high hidden dimension the additive attention outperforms the dot-product one, likely due to the growth of value over iterative computation, pushing the softmax close to small gradient, that's why a scaling coefficient $\\frac{1}{\\sqrt{d_k}}$ appears to balance the dot-product approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f9e157",
   "metadata": {},
   "source": [
    "The Attention is applied basically in 3 parts of a Transformer architecture:\n",
    "\n",
    "- encoder-decoder framework, the query - current embedding state of the decoder and key-value from the hidden-state of encoder performs attention to see how each token in the input attends to each token in the output, this is a basic use of attention in seq2seq opimization\n",
    "\n",
    "- Encoder contains self-attention layer, where V and Q come from same place, the output of previous layers in the layer\n",
    "\n",
    "- Decoder also contains self-attention layer to retain the auto-regressive nature of the model; however, this is more strict as previous hidden-states are marked to prevent the model learn feature in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ec2386",
   "metadata": {},
   "source": [
    "#### Position-wise FFN\n",
    "\n",
    "Another layer of the Transformer Encoder is the Position-wise FeedForward Network, which applies transformation to each invididual token in the sentence (by position) \n",
    "\n",
    "$$FFN(X) = max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "where $W_1, W_2, b_1, b_2$ are trainable parameters\n",
    "\n",
    "this is interpreted as a ReLU activation of 1 linear transformation of input, then a second Linear transformation of the ReLU, creating a 2 layer MLP\n",
    "\n",
    "the dimension of input and output is typically hidden_dim = 512 and inner layer has dimensions 2048 (the transformation layer inside ReLU)\n",
    "\n",
    "$W_1$ (512 x 2048)\n",
    "\n",
    "$W_2$ (2048x512)\n",
    "\n",
    "this is simply a high-dimensional representation of the token for processing before returning the original dimension\n",
    "\n",
    "Note that the linear transformation uses the same parameters across positions in a layer (as a kernel size 1) but not equal between layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a57a8",
   "metadata": {},
   "source": [
    "#### Positional Encoding\n",
    "\n",
    "As Transformer architecture doesn't have recurrent and convolution so the model needs something to utilize the sequence, we need to inject information about the relative position of the token in the sequence. \n",
    "\n",
    "The positional encoding have the same dimension $d_{model}$ as the embeddings, the original Transformer model applies the sine/ cosine functions\n",
    "\n",
    "$$PE_{pos, 2i} = sin(\\frac{pos}{1000^{\\frac{2i}{d_{model}}}})$$\n",
    "\n",
    "$$PE_{pos, 2i+1} = cos(\\frac{pos}{1000^{\\frac{2i}{d_{model}}}})$$\n",
    "\n",
    "The larger order of dimensions, the smaller frequency of the sinusoidal representation, that'll ensure that words will be less likely to have different projection value\n",
    "\n",
    "where pos is the position and i is the hidden_dim of the model\n",
    "\n",
    "This ensures that positions that are close together will have near encodings (due to sinusoidal representation) and the model can also extrapolate to longer sequences (beyone what it's trained on) based on known tokens\n",
    "\n",
    "This is used in Transformer by adding the positional encoding to the token embedding\n",
    "\n",
    "x = token_embedding + positional_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f188854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "head_dim = d_model // num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9215bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch, seq_len, 512) â†’ (batch, num_heads, seq_len, head_dim)\n",
    "Q = Q.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "K = K.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "V = V.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efb0b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "ln = nn.LayerNorm(512)\n",
    "x = torch.randn(2, 5, 512)\n",
    "y = ln(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1132e82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
